| Paper Title                                |    DOI/Link                                    | Question                                                                              | Related Work                                                               | Solution                                                                             | Experimental Design                                                               |Method                                                                                                                     | Analysis                                                                                                                                                                                                                         | Results                                                                                                            | Gap                                                                                                                                                          |             
| ------------------------------------------|-------------------------------------------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------- |--------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------| ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| An Emperical study ofmemorization in NLP  | https://aclanthology.org/2022.acl-long.434.pdf  |This paper addresses                                                                    1. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.                              This paper used three different NLP taks                                              The training instances are identified                                                Experiments were conducted on three datasets:                                                                              In the first set of experiments, the self-influence-based memorization                                                                                                                                                            The paper emperically examines a recently proposed long-tailed thory in the context                                   The attribution method can be a tool to help model developers better understand the memorization behaviors of a model and possibly further improve the model.
                                                                                               a gap about a long tailed theory which explains memorization behavior                      Character-level convolutional networks for text classification.              to check if the long-tail theory holds. The                                          that are memorized by a trained deep learning model                                  1. SST-2, this dataset is used for sentence-level binary sentiment                                                         scoring function is used to rank training instances. Two kinds of checking                                                                                                                                                         of NLP. Sentiment classification, natural language inference, and text classification
                                                                                               In Advances in Neural Information Processing Systems                                    2. Satrajit Chatterjee. 2018. Learning and memorization in International         experiments show that top-ranked memorized training instances                       and then see if they are atypical. Secondly, we check whether                           classification. 2. SNL- This is used for natural language infernece.                                                    are performed: qualitative evaluation and quantitative measures of typicality on sentiment                                                                                                                                         was used to check the validity of the long tailed theory in NLP. A memorization technique of attribution method
                                                                                               in deep learning models. This has not been emperically verified in the context of NLP.     Conference on Machine Learning.                                               lead to a more serious drop in test accuracy compared with removing                 removing the memorized training instances would lead to more better                     It aims to predict entailment relation, between a premise and hypothesis.                                               analysis because annotations are available on the dataset and it helps to define some form of typicality.                                                                                                                           was proposed to reveal which parts of the instance are being memorized.
                                                                                               This paper emperically shows that the memorization attribution method                   3. Vitaly Feldman. 2020. Does learning require memorization?                     training instances only. Furthermore, we develop an attribution method              performance drop on the test data than removing a random sample of training             3. Yahaoo answers- This is a question-answer pair, categorized on 10 topic based                                        For SST-2, the top- ranked memorized instancs are atypical in two ways: the first is based on a heuristic metric, we check 
                                                                                               is faithful and share our interesting finding that the top memorized                       A short tale about a long tail. In Annual ACM SIGACT                          to better understand why a training instance is memorized. The paper                instances. The paper wants to pinpoint which parts of a memorized instance                  classes. For the NLP-tasks, we adopt a Distill-BERT model, that consists of                                          the percentage of positive phrases in an instance, where phrase level sentiment polarity labels are from the annotation provided by SST-2. The second one is we manually inspect the top ranked and the 
                                                                                               parts of a training instance tend to be features negatively correlated                     symposium on Theory of Computing.                                             emperically shows that our memorization attribution method id faithful              are most critical for memorization. The idea of integrated gradients is followed            6 transformer layers, where each layer consists of 12 attention heads. The                                            bottom ranked training instances based on the memorizations score and use human knowledge to judge whether the top ranked ones are atypical while the bottom ranked ones are typical. 
                                                                                               with the class label.                                                                   4. Vitaly Feldman and Chiyuan Zhang. 2020 What neural                            and share our interesting finding that the top memorized parts of a training        and formula is derived to compute memorization attribution.                                 final hidden state of the CLS token is used for classification. In the first set of                                   For SNLI, manual inspection is done of the top ranked and bottom ranked training instances. The experiments emperically validate the long tailed 
                                                                                                                                                                                          networks memorize and why. Discovering the long                               instance tend to be featured negatively corelated with class label.                                                                                                             experiments, the self infleunce based memorization scoring function is used to rank training                          theory on the three NLP datasets, showing memorization is important for generalization and the attribution method can be a tool to help model developers understand the memorization behaviors of a model and improve on it. 
                                                                                                                                                                                          tail via infleucne estimation. In Advances in Neural Information                                                                                                                                                                                              instances. Two kinds of checking are performed: qualitative and measure of typicality on sentiment                            
                                                                                                                                                                                          Processing Systems.                                                                                                                                                                                                                                           analysis because annotations are available on the dataset and it helps to define some form of typicality.                                                                                            
                                                                                                                                                                                       5. Pan Wei Koh and Percy Liang 2017. Understanding black-box predictions 
                                                                                                                                                                                          via influence functions. In International Conference on Machine Learning. theory on the three NLP datasets, showing memorization is important for generalization and the attribution method can be a tool to help model developers understand the memorization behaviors of a model and improve on it. 
